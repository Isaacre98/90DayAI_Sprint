# 🤖 Deep Learning Specialization (Andrew Ng, DeepLearning.AI)

## 📌 Overview

This folder documents my journey through the **Deep Learning Specialization** by Andrew Ng and DeepLearning.AI on Coursera, as part of my [90-Day AI Sprint](../README.md).  

Over the course of 5 intensive modules, I implemented key components of deep learning from scratch, built working models using TensorFlow/Keras, and applied theoretical knowledge in real-world projects.

This specialization has helped me move from foundational ML knowledge into applied deep learning — with a particular focus on CNNs, regularization, optimization, and sequence models.

---

## 🧭 Learning Goals

- Understand the inner workings of neural networks and their mathematical foundations
- Implement core algorithms manually to reinforce comprehension
- Use modern frameworks (TensorFlow/Keras) to build end-to-end models
- Debug training issues and optimize performance
- Apply deep learning to practical use cases (e.g., computer vision)

---

## 📚 Course Breakdown

| Course | Title                                 | Status     | Notes |
|--------|---------------------------------------|------------|-------|
| 1      | Neural Networks and Deep Learning     | ✅ Done     | Implemented forward & backprop from scratch |
| 2      | Improving Deep Neural Networks        | 🔄 In Progress | Regularization, optimization, and tuning |
| 3      | Structuring ML Projects               | ⏳ Queued   | Best practices for project design |
| 4      | Convolutional Neural Networks         | ⏳ Queued   | Vision models & CNNs |
| 5      | Sequence Models (RNNs, LSTMs, NLP)    | ⏳ Queued   | Working with time series, text, and sequences |

---

## 🧪 Featured Projects

### 1️⃣ Binary Image Classifier — *Cats vs. Dogs*  
📁 `Projects/cats_vs_dogs_classifier`

A convolutional neural network built using Keras to classify images of cats and dogs.  
Key highlights:
- Custom CNN architecture
- Image augmentation & regularization
- Visualization of misclassifications

→ [View Project README](./Projects/cats_vs_dogs_classifier/README.md)

---

## 📊 Skills & Concepts Mastered

- ✅ Backpropagation and gradient descent
- ✅ Hyperparameter tuning
- ✅ Weight initialization strategies
- ✅ Dropout, batch norm, and L2 regularization
- ✅ Activation functions (ReLU, sigmoid, softmax)
- ✅ Convolutional filters, pooling, and padding
- ✅ Optimizers (SGD, Adam)
- ⏳ RNNs, GRUs, LSTMs — coming soon

---

## 📈 Reflections & Insights

- **Implementation-first mindset:** Building models from scratch revealed subtle behaviors (e.g., vanishing gradients, convergence issues) that are easy to miss with high-level libraries.
- **Debugging matters:** Early model failures were mostly due to batch size, learning rate, and data preprocessing — not code bugs.
- **Pattern recognition:** Through repetition, I started seeing common training signals (overfitting curves, loss plateaus) and reacting to them faster.

---

## 🔮 What’s Next?

- Complete Courses 3–5 by June 28, 2025
- Publish at least one project for each module (e.g., Emotion Recognition, Music Generator, Named Entity Tagging)
- Try transfer learning on my classifier using pre-trained CNNs (ResNet, MobileNet)
- Write a blog post reflecting on the entire specialization

---

## 🧠 Part of the 90-Day AI Sprint

This work is part of my [90-Day AI Sprint](../README.md), where I learn and build AI tools through a structured, project-based learning plan. Follow my journey and check out my other projects [on GitHub](https://github.com/Isaacre98/90DayAI_Sprint).

---

